{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Means with Iris dataset\n",
    "     In this notebook we are going to be building a rudimentary K-means model with the iris dataset and my sole mathematical and numpy knowledge.\n",
    "\n",
    "\n",
    "## Loading up the dataset and required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()\n",
    "X=iris.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing The Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_initial_centroids(data,k):\n",
    "    np.random.seed(1)\n",
    "    m,n=data.shape[0],data.shape[1]\n",
    "    centroid=np.zeros([k,n])\n",
    "    for i in range(k):\n",
    "        centroid[i]=data[np.random.randint(0,m+1),:]\n",
    "\n",
    "    return centroid\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating and Building our Clusters with our Centroids and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids_cluster(data,centroid,iter_no):\n",
    "    m,n=data.shape[0],data.shape[1]\n",
    "    clusters={}\n",
    "    cluster_start={}\n",
    "    for j in range(iter_no):\n",
    "        for i in range(centroid.shape[0]):\n",
    "            cluster_start[str(i)]=False\n",
    "            clusters[str(i)]=np.zeros((1,n)) #initializing the clusters with existing centroids so that the mean stays\n",
    "        for i in range(m):\n",
    "                dist=(np.sum((data[i]-centroid)**2,axis=1))**(1/2) #calculating euclidean distance of vector from each centroid\n",
    "                min_dist_map=np.argmin(dist) #mapping nearest centroid\n",
    "                if cluster_start[str(min_dist_map)]==False: # for the first iteration within the dataset, we need to find the average between the centroid and the first cluster\n",
    "                    clusters[str(min_dist_map)]=data[i].reshape(1,n) #assign data point to cluster with centroid of minimum euclidean distance\n",
    "                    centroid[min_dist_map]=(centroid[min_dist_map]+data[i].reshape(1,n))/2\n",
    "                    cluster_start[str(min_dist_map)]=True #Turning on the switch will move machine to start stacking the data to clusters instead of assigning the data point like we do when we store data in the cluster for the first time\n",
    "                else:    \n",
    "                    clusters[str(min_dist_map)]=np.vstack([clusters[str(min_dist_map)],data[i]]) #storing vector in cluster with nearest centroid\n",
    "                    centroid[min_dist_map]=np.mean(clusters[str(min_dist_map)],axis=0)  #moving centroid with mean of added vector along with the previous vector in the cluster\n",
    "    \n",
    "    return centroid, clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_cent=get_initial_centroids(X,3)\n",
    "cent,clus=compute_centroids_cluster(X,init_cent,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 Neighbor No. :\n",
      "50\n",
      "Cluster 1 Neighbor No. :\n",
      "40\n",
      "Cluster 2 Neighbor No. :\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "for i in clus:\n",
    "    print('Cluster '+ str(i) + ' Neighbor No. :')\n",
    "    print(clus[i].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0 Neighbor No. :\n",
      "50\n",
      "Cluster 1 Neighbor No. :\n",
      "50\n",
      "Cluster 2 Neighbor No. :\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "for i, j in collections.Counter(iris.target).items():\n",
    "    print('Cluster '+ str(i) + ' Neighbor No. :')\n",
    "    print(j)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the end, we still find that our model missed the mark by 10 data points. Forgive my inabilities, this is the most I could conjure from whatever knowledge I had of python and K-Means theory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
